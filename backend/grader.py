import os
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_mistralai import ChatMistralAI
from dotenv import load_dotenv

# -------------------------- Setup --------------------------


load_dotenv()

API_KEY = os.getenv("API_KEY")
AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT")


# -------------------------- SQL Query Sense Grader --------------------------


class GradeSQLSense(BaseModel):
    """Binary score for semantic soundness of a SQL query."""

    binary_score: str = Field(
        description="SQL query is semantically sound and logically consistent, 'yes' or 'no'"
    )


def get_sql_sense_grader():
    system_prompt = """You are a grader assessing the **semantic and logical soundness** of a SQL query that has been generated by an AI model. Assume the SQL runs without error.

Evaluate the following (but not limited to):
- Are JOINs between tables meaningful? (e.g. joined on related keys)
- Are GROUP BY and aggregation functions used logically? (e.g. SUM, AVG used on numeric fields, not IDs)
- Are WHERE/HAVING clauses reasonable? (e.g. no illogical filters)
- Are the chosen tables the most relevant for the intended question? Could the model have chosen better?
- Are column references appropriate for the intended question?
- Are subqueries used meaningfully?
- Any sign the query would return misleading or nonsensical results?

Write out your reasoning internally before submitting an answer.

If the query makes sense and reflects sound logic in structure and intention, respond 'yes'. Otherwise, respond 'no'."""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "human",
                "User question: {question}\n\nSQL query: \n\n{query}\n\nTables that were available to the model: \n\n{available_tables}",
            ),
        ]
    )

    llm = ChatMistralAI(
        api_key=API_KEY,
        max_tokens=100,
        temperature=0,
        model_name="mistral-large-latest",
    )
    return prompt | llm.with_structured_output(GradeSQLSense, method="function_calling")


# -------------------------- Data Sense Grader --------------------------


class GradeDataSense(BaseModel):
    """Binary score to assess whether the data result is sensible and logically consistent."""

    binary_score: str = Field(
        description="Data result is logically consistent and makes sense in context, 'yes' or 'no'"
    )


def get_data_sense_grader():
    system_prompt = """
You are a grader assessing whether a SQL query result makes sense in context of the user question.
Consider if the data seems implausible or suspicious (e.g., unexpected magnitude, out-of-range values, empty or overloaded results).
Use common sense and knowledge of typical data behavior. You are not evaluating accuracy but sensibility.

Write out your reasoning internally before submitting an answer.

Give a binary score 'yes' or 'no'. 'Yes' means the result appears plausible and logically consistent.
"""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "human",
                "User question: \n\n{question}\n\nSQL result: {data_result}",
            ),
        ]
    )

    llm = ChatMistralAI(
        api_key=API_KEY,
        max_tokens=100,
        temperature=0,
        model_name="mistral-large-latest",
    )
    return prompt | llm.with_structured_output(
        GradeDataSense, method="function_calling"
    )


# -------------------------- Hallucination Grader (Data Grounding) --------------------------


class GradeDataHallucination(BaseModel):
    """Binary score for hallucination check against retrieved data."""

    binary_score: str = Field(
        description="LLM output is grounded in the SQL result, 'yes' or 'no'"
    )


def get_data_hallucination_grader():
    system_prompt = """
You are a grader assessing whether a model-generated answer is grounded in a SQL query result.
The answer should reflect or be directly supported by the result content (fields, values, aggregations).

Write out your reasoning internally before submitting an answer.

Give a binary score 'yes' or 'no'. 'Yes' means the answer is consistent with the SQL result.
"""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "human",
                "SQL result: \n\n{data_result}\n\nLLM answer: {generation}",
            ),
        ]
    )

    llm = ChatMistralAI(
        api_key=API_KEY,
        max_tokens=100,
        temperature=0,
        model_name="mistral-large-latest",
    )
    return prompt | llm.with_structured_output(
        GradeDataHallucination, method="function_calling"
    )


# -------------------------- Answer Relevance Grader --------------------------


class GradeAnswerRelevance(BaseModel):
    """Binary score to assess if LLM answer addresses the user question."""

    binary_score: str = Field(
        description="Answer addresses the user question, 'yes' or 'no'"
    )


def get_answer_relevance_grader():
    system_prompt = """
You are a grader assessing whether a model's answer addresses a user question.
Focus on semantic alignment and completeness. Even if partially correct, it must clearly aim to answer the intent.

Write out your reasoning internally before submitting an answer.

Give a binary score 'yes' or 'no'. 'Yes' means the answer addresses the question.
"""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "human",
                "User question: \n\n{question}\n\nLLM answer: {generation}",
            ),
        ]
    )

    llm = ChatMistralAI(
        api_key=API_KEY,
        max_tokens=100,
        temperature=0,
        model_name="mistral-large-latest",
    )
    return prompt | llm.with_structured_output(
        GradeAnswerRelevance, method="function_calling"
    )
